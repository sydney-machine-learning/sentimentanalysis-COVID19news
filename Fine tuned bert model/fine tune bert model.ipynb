{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>=3.8->textblob) (8.1.6)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>=3.8->textblob) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>=3.8->textblob) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: demoji in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (1.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading all: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "import csv\n",
    "import string\n",
    "!pip3 install textblob\n",
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "!pip3 install demoji\n",
    "nltk.download('all')\n",
    "import demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "zsh:1: no matches found: transformers[torch]\n"
     ]
    }
   ],
   "source": [
    "#Set up the environment\n",
    "!pip3 install -q transformers datasets\n",
    "#Download the accelarator\n",
    "!pip3 install transformers[torch] -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torchtext==0.6.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.6.0)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.6.0) (4.65.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.6.0) (2.31.0)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.6.0) (2.2.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.6.0) (1.25.1)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.6.0) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torchtext==0.6.0) (0.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchtext==0.6.0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchtext==0.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchtext==0.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->torchtext==0.6.0) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (4.10.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (1.12)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch --upgrade\n",
    "!pip3 install torchtext==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "SEED = 1024\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import operator\n",
    "\n",
    "from sklearn.metrics import hamming_loss, jaccard_score, label_ranking_average_precision_score, f1_score\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import demoji\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "class preprocess():\n",
    "    def __init__(self, df, contractions, otherContractions):\n",
    "        self.df = df\n",
    "        self.contractions = contractions\n",
    "        self.otherContractions = otherContractions\n",
    "    \n",
    "    def lower(self, tweet):\n",
    "        return tweet.lower()\n",
    "    \n",
    "    def abbreviate(self, tweet):\n",
    "        tweet = tweet.split(' ')\n",
    "        j = 0\n",
    "        for str_ in tweet:\n",
    "            fileName = '/Users/neo/Documents/bert/Abbreviations.txt'\n",
    "            accessMode = 'r'\n",
    "            with open(fileName, accessMode) as csvfile:\n",
    "                dataFromFile = csv.reader(csvfile, delimiter = '=')\n",
    "                str_ = re.sub('[^a-zA-Z0-9-_.]', '', str_)\n",
    "                for row in dataFromFile:\n",
    "                    if str_.upper() == row[0]:\n",
    "                        tweet[j] = row[1]\n",
    "                csvfile.close()\n",
    "            j += 1\n",
    "        return ' '.join(tweet)\n",
    "    \n",
    "    def expand(self, tweet):\n",
    "        for word in tweet.split():\n",
    "            if word in self.contractions.keys():\n",
    "                tweet = tweet.replace(word, self.contractions[word])\n",
    "            elif word in self.otherContractions.keys():\n",
    "                tweet = tweet.replace(word, self.otherContractions[word])\n",
    "        return tweet\n",
    "    \n",
    "    def emoji2text(self, tweet):\n",
    "        emojis = demoji.findall(tweet)\n",
    "        new_tweet = []\n",
    "        for word in tweet.split():\n",
    "            if word in emojis.keys():\n",
    "                tweet = tweet.replace(word, emojis[word])\n",
    "                new_tweet.append(emojis[word])\n",
    "            wordmojis = demoji.findall(word)\n",
    "            for char in word:\n",
    "                if char in wordmojis.keys():\n",
    "                    tweet = tweet.replace(word, wordmojis[char])\n",
    "        \n",
    "        return tweet\n",
    "\n",
    "    def remove_hashtags(self, tweet):\n",
    "        return re.sub(r'\\#w+', '', tweet)\n",
    "    \n",
    "    def remove_mentions(self, tweet):\n",
    "        for word in tweet.split():\n",
    "            if word[0] == '@':\n",
    "                tweet = tweet.replace(word, '')\n",
    "        return tweet\n",
    "\n",
    "    def remove_punctuations(self, tweet):\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')\n",
    "        return tweet.translate(trantab)\n",
    "    \n",
    "    def preprocess_tweet(self, tweet):\n",
    "        tweet = self.lower(tweet)\n",
    "        tweet = self.abbreviate(tweet)\n",
    "        tweet = self.expand(tweet)\n",
    "        tweet = self.emoji2text(tweet)\n",
    "        tweet = self.remove_mentions(tweet)\n",
    "        tweet = self.remove_hashtags(tweet)\n",
    "        tweet = self.remove_punctuations(tweet)\n",
    "        return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Optimistic</th>\n",
       "      <th>Thankful</th>\n",
       "      <th>Empathetic</th>\n",
       "      <th>Pessimistic</th>\n",
       "      <th>Anxious</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Annoyed</th>\n",
       "      <th>Denial</th>\n",
       "      <th>Official report</th>\n",
       "      <th>Joking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>A glass of wine keeps the corona away- DRAKE. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>Can Anyone tell me if you took the flu shot la...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>Btw producers send me beats Im working on musi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>When someone you know.. apart of your family d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>Dear soccer, I really miss you ,please come ba...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>new home remedy to treat coronavirus! tested b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>When Xavier Wulf does an Attack on Titan tape ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>Mouthwash is hand san for your mouth and I don...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>Yes all of them ! n France 1.000 Christians to...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>Update I destroyed the tire honestly if I get ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ID                                              Tweet  \\\n",
       "0  1.245140e+18  A glass of wine keeps the corona away- DRAKE. ...   \n",
       "1  1.245140e+18  Can Anyone tell me if you took the flu shot la...   \n",
       "2  1.245140e+18  Btw producers send me beats Im working on musi...   \n",
       "3  1.245140e+18  When someone you know.. apart of your family d...   \n",
       "4  1.245140e+18  Dear soccer, I really miss you ,please come ba...   \n",
       "5  1.245140e+18  new home remedy to treat coronavirus! tested b...   \n",
       "6  1.245140e+18  When Xavier Wulf does an Attack on Titan tape ...   \n",
       "7  1.245140e+18  Mouthwash is hand san for your mouth and I don...   \n",
       "8  1.245140e+18  Yes all of them ! n France 1.000 Christians to...   \n",
       "9  1.245140e+18  Update I destroyed the tire honestly if I get ...   \n",
       "\n",
       "   Optimistic  Thankful  Empathetic  Pessimistic  Anxious  Sad  Annoyed  \\\n",
       "0           1         0           0            0        0    0        0   \n",
       "1           0         0           0            0        1    0        0   \n",
       "2           1         0           0            0        0    0        0   \n",
       "3           0         0           0            0        0    1        0   \n",
       "4           0         0           0            0        0    1        1   \n",
       "5           1         0           0            0        0    0        0   \n",
       "6           0         0           0            0        0    0        0   \n",
       "7           0         0           0            0        0    1        0   \n",
       "8           0         0           0            0        0    0        1   \n",
       "9           0         0           0            1        0    1        0   \n",
       "\n",
       "   Denial  Official report  Joking  \n",
       "0       0                0       1  \n",
       "1       0                0       0  \n",
       "2       0                0       1  \n",
       "3       0                0       0  \n",
       "4       0                0       0  \n",
       "5       1                0       1  \n",
       "6       0                0       1  \n",
       "7       0                0       1  \n",
       "8       0                1       0  \n",
       "9       0                0       0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senwave = pd.read_csv(\"/Users/neo/Documents/bert/labeledEn.csv\")\n",
    "senwave.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = { \n",
    "\"ain't\": \"am not\", \"aren't\": \"are not\", \"can't\": \"cannot\", \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\", \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he had\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \"he'll've\": \"he will have\", \"he's\": \"he is\",\n",
    "\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\", \"i'll've\": \"i will have\", \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\", \"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\", \"so's\": \"so is\", \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\", \"there's\": \"there is\", \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "\"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\", \"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\", \"y'all've\": \"you all have\", \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\", \"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractionsWithAnotherInvertedComma = { \n",
    "\"ain’t\": \"am not\", \"aren’t\": \"are not\", \"can’t\": \"cannot\", \"can’t’ve\": \"cannot have\", \"’cause\": \"because\", \"could’ve\": \"could have\", \"couldn’t\": \"could not\",\n",
    "\"couldn’t’ve\": \"could not have\", \"didn’t\": \"did not\", \"doesn’t\": \"does not\", \"don’t\": \"do not\", \"hadn’t\": \"had not\", \"hadn’t’ve\": \"had not have\",\n",
    "\"hasn’t\": \"has not\", \"haven’t\": \"have not\", \"he’d\": \"he had\", \"he’d’ve\": \"he would have\", \"he’ll\": \"he will\", \"he’ll’ve\": \"he will have\", \"he’s\": \"he is\",\n",
    "\"how’d\": \"how did\", \"how’d’y\": \"how do you\", \"how’ll\": \"how will\", \"how’s\": \"how is\", \"i’d\": \"i would\", \"i’d’ve\": \"i would have\",\n",
    "\"i’ll\": \"i will\", \"i’ll’ve\": \"i will have\", \"i’m\": \"i am\", \"i’ve\": \"i have\", \"isn’t\": \"is not\", \"it’d\": \"it would\",\n",
    "\"it’d’ve\": \"it would have\", \"it’ll\": \"it will\", \"it’ll’ve\": \"it will have\", \"it’s\": \"it is\", \"let’s\": \"let us\",\n",
    "\"ma’am\": \"madam\", \"mayn’t\": \"may not\", \"might’ve\": \"might have\", \"mightn’t\": \"might not\", \"mightn’t’ve\": \"might not have\", \"must’ve\": \"must have\", \"mustn’t\": \"must not\",\n",
    "\"mustn’t’ve\": \"must not have\", \"needn’t\": \"need not\", \"needn’t’ve\": \"need not have\", \"o’clock\": \"of the clock\", \"oughtn’t\": \"ought not\", \"oughtn’t’ve\": \"ought not have\",\n",
    "\"shan’t\": \"shall not\", \"shan’t’ve\": \"shall not have\", \"she’d\": \"she would\", \"she’d’ve\": \"she would have\", \"she’ll\": \"she will\",\n",
    "\"she’ll’ve\": \"she will have\", \"she’s\": \"she is\", \"should’ve\": \"should have\", \"shouldn’t\": \"should not\", \"shouldn’t’ve\": \"should not have\",\n",
    "\"so’ve\": \"so have\", \"so’s\": \"so is\", \"that’d\": \"that would\", \"that’d’ve\": \"that would have\", \"that’s\": \"that is\", \"there’d\": \"there would\",\n",
    "\"there’d’ve\": \"there would have\", \"there’s\": \"there is\", \"they’d\": \"they would\", \"they’d’ve\": \"they would have\", \"they’ll\": \"they will\",\n",
    "\"they’ll’ve\": \"they will have\", \"they’re\": \"they are\", \"they’ve\": \"they have\", \"to’ve\": \"to have\", \"wasn’t\": \"was not\", \"we’d\": \"we would\",\n",
    "\"we’d’ve\": \"we would have\", \"we’ll\": \"we will\", \"we’ll’ve\": \"we will have\", \"we’re\": \"we are\", \"we’ve\": \"we have\", \"weren’t\": \"were not\", \"what’ll\": \"what will\",\n",
    "\"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\",\n",
    "\"when’ve\": \"when have\", \"where’d\": \"where did\", \"where’s\": \"where is\", \"where’ve\": \"where have\", \"who’ll\": \"who will\", \"who’ll’ve\": \"who will have\",\n",
    "\"who’s\": \"who is\", \"who’ve\": \"who have\", \"why’s\": \"why is\", \"why’ve\": \"why have\", \"will’ve\": \"will have\", \"won’t\": \"will not\", \"won’t’ve\": \"will not have\",\n",
    "\"would’ve\": \"would have\", \"wouldn’t\": \"would not\", \"wouldn’t’ve\": \"would not have\", \"y’all\": \"you all\", \"y’all’d\": \"you all would\", \"y’all’d’ve\": \"you all would have\",\n",
    "\"y’all’re\": \"you all are\", \"y’all’ve\": \"you all have\", \"you’d\": \"you would\", \"you’d’ve\": \"you would have\", \"you’ll\": \"you will\", \"you’ll’ve\": \"you will have\",\n",
    "\"you’re\": \"you are\", \"you’ve\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_class = preprocess(senwave, contractions, contractionsWithAnotherInvertedComma)\n",
    "senwave['Tweet'] = senwave['Tweet'].apply(lambda x : pp_class.preprocess_tweet(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_coverage(vocab, embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "        \n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key = operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "def build_vocab(sentences, verbose = True):\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path, encoding = \"utf-8\") as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 2196008 word vectors \n"
     ]
    }
   ],
   "source": [
    "GLOVE_EMBEDDING_FILE = '/Users/neo/Documents/bert/glove.840B.300d.txt'\n",
    "glove_embeddings = load_embeddings(GLOVE_EMBEDDING_FILE)\n",
    "print(f'loaded {len(glove_embeddings)} word vectors ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb11b89321c41dfbf344f0117083cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2289292522dc4734985126c417b43cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 86.65% of vocab\n",
      "Found embeddings for  96.72% of all text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('covid19', 2240),\n",
       " ('covid', 408),\n",
       " ('aprilfoolsday', 135),\n",
       " ('stayathome', 73),\n",
       " ('aprilfools', 48),\n",
       " ('coronavirusoutbreak', 34),\n",
       " ('stayhome', 29),\n",
       " ('quarantinelife', 26),\n",
       " ('tablighijamaat', 25),\n",
       " ('fauci', 22)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = build_vocab(list(senwave['Tweet'].apply(lambda x : x.split())))\n",
    "oov = check_coverage(vocab, glove_embeddings)\n",
    "oov[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordreplace(tweet):\n",
    "    tweet = tweet.replace(\"indiavscorona\", \"india versus coronavirus\")\n",
    "    tweet = tweet.replace(\"outbreakindia\", \"outbreak india\")\n",
    "    tweet = tweet.replace(\"real”\", \"real\")\n",
    "    tweet = tweet.replace(\"mutra\", \"urine\")\n",
    "    tweet = tweet.replace(\"fakenews\", \"fake news\")\n",
    "    tweet = tweet.replace(\"“omg\", \"oh my god\")\n",
    "    tweet = tweet.replace(\"“damn\", \"damn\")\n",
    "    tweet = tweet.replace(\"god’s\", \"gods\")\n",
    "    tweet = tweet.replace(\"lockdownextension\", \"lockdown extension\")\n",
    "    tweet = tweet.replace(\"कोरोना\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"indiathanks\", \"india thanks\")\n",
    "    tweet = tweet.replace(\"coronacoronavirus\", \"coronavirus\")\n",
    "    tweet = tweet.replace('coronavirusinsa', \"coronavirus in south africa\")\n",
    "    tweet = tweet.replace('coronaviruscanada', 'coronavirus canada')\n",
    "    tweet = tweet.replace('coronavirusau', 'coronavirus australia')\n",
    "    tweet = tweet.replace('coronavirusaus', 'coronavirus australia')\n",
    "    tweet = tweet.replace('cuomoprimetime', 'new york governor prime time')\n",
    "    tweet = tweet.replace('letsfightcoronavirus', 'let us fight coronavirus')\n",
    "    tweet = tweet.replace(\"covid19\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"covid\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"aprilfoolsday\", \"april fools day\")\n",
    "    tweet = tweet.replace(\"covidー19\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"stayathome\", \"stay at home\")\n",
    "    tweet = tweet.replace(\"“april\", \"april\")\n",
    "    tweet = tweet.replace(\"“i\", \"i\")\n",
    "    tweet = tweet.replace(\"aprilfools\", \"april fools\")\n",
    "    tweet = tweet.replace(\"coronavirusoutbreak\", \"coronavirus outbreak\")\n",
    "    tweet = tweet.replace(\"virusー19\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"fool’s\", \"fools\")\n",
    "    tweet = tweet.replace(\"what’s\", \"what is\")\n",
    "    tweet = tweet.replace(\"coronavirus”\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"fools”\", \"fools\")\n",
    "    tweet = tweet.replace(\"stayhome\", \"stay home\")\n",
    "    tweet = tweet.replace(\"quarantinelife\", \"quarantine life\")\n",
    "    tweet = tweet.replace(\"tablighijamaat\", \"muslims\")\n",
    "    tweet = tweet.replace(\"corona”\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"fauci\", \"physician\")\n",
    "    tweet = tweet.replace(\"april’s\", \"april\")\n",
    "    tweet = tweet.replace(\"pmkcallscurfewextension\", \"prime minister calls for curfew extension\")\n",
    "    tweet = tweet.replace(\"“virus\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"virus”\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"“corona\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"coronavirustruth\", \"coronavirus truth\")\n",
    "    tweet = tweet.replace(\"socialdistancing\", \"social distancing\")\n",
    "    tweet = tweet.replace(\"homestaysafe\", \"home stay safe\")\n",
    "    tweet = tweet.replace(\"“coronavirus\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"coronavirusupdate\", \"coronavirus update\")\n",
    "    tweet = tweet.replace(\"virusvirus\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"coronaviruspandemic\", \"coronavirus pandemic\")\n",
    "    tweet = tweet.replace(\"thelockdown\", \"the lockdown\")\n",
    "    tweet = tweet.replace(\"nizamuddin\", \"delhi\")\n",
    "    tweet = tweet.replace(\"trump’s\", \"donald trump\")\n",
    "    tweet = tweet.replace(\"“the\", \"the\")\n",
    "    tweet = tweet.replace(\"virus2019\", \"coronavirus\")\n",
    "    tweet = tweet.replace(\"indiafightscorona\", \"india fights coronavirus\")\n",
    "    tweet = tweet.replace(\"homesavelives\", \"home save lives\")\n",
    "    tweet = tweet.replace(\"everyone’s\", \"everyone\")\n",
    "    tweet = tweet.replace(\"coronariskforprisoners\", \"coronavirus risk for prisoners\")\n",
    "    tweet = tweet.replace(\"coronavirususa\", \"coronavirus usa\")\n",
    "    tweet = tweet.replace(\"tablighi\", \"mosque\")\n",
    "    tweet = tweet.replace(\"delhimarkaz\", \"delhi mosque\")\n",
    "    tweet = tweet.replace(\"coronajihad\", \"coronavirus struggle\")\n",
    "    tweet = tweet.replace(\"coronajihaad\", \"coronavirus struggle\")\n",
    "    tweet = tweet.replace(\"aprilfool\", \"april fool\")\n",
    "    tweet = tweet.replace(\"trumppressconference\", \"trump press conference\")\n",
    "    tweet = tweet.replace(\"i’m\", \"i am\")\n",
    "    tweet = tweet.replace(\"tigerking\", \"tiger king\")\n",
    "    tweet = tweet.replace(\"it’s\", \"it is\")\n",
    "    tweet = tweet.replace(\"trumpvirus\", \"trump virus\")\n",
    "    tweet = tweet.replace(\"today’s\", \"today is\")\n",
    "    tweet = tweet.replace(\"“you\", \"you\")\n",
    "    tweet = tweet.replace(\"“a\", \"a\")\n",
    "    tweet = tweet.replace(\"fools’\", \"fools\")\n",
    "    tweet = tweet.replace(\"rtgnews\", \"news\")\n",
    "    tweet = tweet.replace(\"19india\", \"india\")\n",
    "    tweet = tweet.replace(\"coronavirusindia\", \"coronavirus india\")\n",
    "    tweet = tweet.replace(\"y’all\", \"you all\")\n",
    "    tweet = tweet.replace(\"मीडिया\", \"media\")\n",
    "    tweet = tweet.replace(\"here’s\", \"here is\")\n",
    "    tweet = tweet.replace(\"“we\", \"we\")\n",
    "    tweet = tweet.replace(\"“fuck\", \"fuck\")\n",
    "    tweet = tweet.replace(\"flattenthecurve\", \"flatten the curve\")\n",
    "    tweet = tweet.replace(\"jammuandkashmir\", \"jammu and kashmir\")\n",
    "    tweet = tweet.replace(\"chriscuomo\", \"new york governor\")\n",
    "    tweet = tweet.replace(\"‘april\", \"april\")\n",
    "    tweet = tweet.replace(\"dranbumani\", \"doctor\")\n",
    "    tweet = tweet.replace(\"tndemandsmasstesting\", \"tamil nadu demands mass testing\")\n",
    "    tweet = tweet.replace(\"tabligi\", \"muslims\")\n",
    "    tweet = tweet.replace(\"don’t\", \"do not\")\n",
    "    tweet = tweet.replace(\"वायरस\", \"virus\")\n",
    "    tweet = tweet.replace(\"letsfightvirus\", \"let us fight virus\")\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "senwave['Tweet'] = senwave['Tweet'].apply(lambda x : wordreplace(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "sen_train, sen_test = train_test_split(senwave, train_size = 0.9, random_state = 1024)\n",
    "\n",
    "sen_train.to_csv(\"train.csv\", index = False)\n",
    "sen_test.to_csv(\"test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Optimistic</th>\n",
       "      <th>Thankful</th>\n",
       "      <th>Empathetic</th>\n",
       "      <th>Pessimistic</th>\n",
       "      <th>Anxious</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Annoyed</th>\n",
       "      <th>Denial</th>\n",
       "      <th>Official report</th>\n",
       "      <th>Joking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>1.245140e+18</td>\n",
       "      <td>minister for agriculture mahendra reddy to hol...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID                                              Tweet  \\\n",
       "560  1.245140e+18  minister for agriculture mahendra reddy to hol...   \n",
       "\n",
       "     Optimistic  Thankful  Empathetic  Pessimistic  Anxious  Sad  Annoyed  \\\n",
       "560           0         0           0            0        0    0        0   \n",
       "\n",
       "     Denial  Official report  Joking  \n",
       "560       0                1       0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from torchtext.data import TabularDataset, Field\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenizer(tweet):\n",
    "    tweet = re.sub(r'[\\n]', ' ', tweet)\n",
    "    return [tok.text for tok in nlp.tokenizer(tweet)]\n",
    "\n",
    "TWEET = Field(sequential=True, lower=True, tokenize=tokenizer)\n",
    "LABEL = Field(sequential=False, use_vocab=False)\n",
    "\n",
    "dataFields = [(\"ID\", None), (\"Tweet\", TWEET), (\"Optimistic\", LABEL), (\"Thankful\", LABEL),\n",
    "              (\"Empathetic\", LABEL), (\"Pessimistic\", LABEL), (\"Anxious\", LABEL), (\"Sad\", LABEL),\n",
    "              (\"Annoyed\", LABEL), (\"Denial\", LABEL), (\"Official report\", LABEL),\n",
    "              (\"Surprise\", LABEL), (\"Joking\", LABEL)]\n",
    "\n",
    "train_dataset, test_dataset = TabularDataset.splits(\n",
    "    path = '/Users/neo/Documents/bert/', train = 'train.csv', test = 'test.csv', format = 'csv', fields = dataFields, skip_header = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples : 9000\n",
      " Number of testing samples : 1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples : {}\\n Number of testing samples : {}\".format(len(train_dataset), len(test_dataset)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "TWEET.build_vocab(train_dataset, vectors = 'glove.840B.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a glass of wine keeps the corona away  drake  ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>can anyone tell me if you took the flu shot la...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>By The Way producers send me beats im working ...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>when someone you know   apart of your family d...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dear soccer  i really miss you  please come ba...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0  a glass of wine keeps the corona away  drake  ...   \n",
       "1  can anyone tell me if you took the flu shot la...   \n",
       "2  By The Way producers send me beats im working ...   \n",
       "3  when someone you know   apart of your family d...   \n",
       "4  dear soccer  i really miss you  please come ba...   \n",
       "\n",
       "                             list  \n",
       "0  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "1  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "2  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "3  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "4  [0, 0, 0, 0, 0, 1, 1, 0, 0, 0]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = senwave.drop(['ID'], axis = 1)\n",
    "df['list'] = df[df.columns[1:12]].values.tolist()\n",
    "new_df = df[['Tweet', 'list']].copy()\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "MAX_LEN = 200 #based on length of tweets\n",
    "TRAIN_BATCH_SIZE = 1\n",
    "VALID_BATCH_SIZE = 1\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 1e-05 #tried 1e-03, 1e-04, 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataframe = dataframe\n",
    "        self.tweet = dataframe['Tweet']\n",
    "        self.targets = self.dataframe.list\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tweet)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        tweet = str(self.tweet[index])\n",
    "        tweet = \" \".join(tweet.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            None,\n",
    "            add_special_tokens = True,\n",
    "            max_length = self.max_len,\n",
    "            pad_to_max_length = True,\n",
    "            return_token_type_ids = True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "\n",
    "        return {\n",
    "            'ids' : torch.tensor(ids, dtype = torch.long),\n",
    "            'mask' : torch.tensor(mask, dtype = torch.long),\n",
    "            'token_type_ids' : torch.tensor(token_type_ids, dtype = torch.long),\n",
    "            'targets' : torch.tensor(self.targets[index], dtype = torch.float)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = sen_train.drop(['ID'], axis = 1)\n",
    "train_dataset['list'] = train_dataset[train_dataset.columns[1:12]].values.tolist()\n",
    "train_df = train_dataset[['Tweet', 'list']].copy()\n",
    "train_df = train_df.reset_index(drop = True)\n",
    "\n",
    "test_dataset = sen_test.drop(['ID'], axis = 1)\n",
    "test_dataset['list'] = test_dataset[test_dataset.columns[1:12]].values.tolist()\n",
    "test_df = test_dataset[['Tweet', 'list']].copy()\n",
    "test_df = test_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = CustomDataset(train_df, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (layer1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Dropout(p=0.3, inplace=False)\n",
       "  (layer3): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "class BERT(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.layer2 = torch.nn.Dropout(0.3)\n",
    "        self.layer3 = torch.nn.Linear(768, 10)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids, return_dict = False):\n",
    "        unw, out_1 = self.layer1(ids, attention_mask = mask, token_type_ids = token_type_ids)[0], self.layer1(ids, attention_mask = mask, token_type_ids = token_type_ids)[1]\n",
    "        out_2 = self.layer2(out_1)\n",
    "        out_final = self.layer3(out_2)\n",
    "        return out_final\n",
    "\n",
    "model = BERT()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Initialize total loss for this epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    # Iterate over the training data loader\n",
    "    for batch_idx, data in enumerate(tqdm(training_loader), 0):\n",
    "        # Move input tensors to the appropriate device (GPU or CPU)\n",
    "        ids = data['ids'].to(device)\n",
    "        mask = data['mask'].to(device)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device)\n",
    "\n",
    "        # Forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(ids, mask,token_type_ids)\n",
    "\n",
    "        # Print the shapes of outputs and targets for debugging\n",
    "        # print(\"Output shape:\", outputs.shape)\n",
    "        # print(\"Target shape:\", targets.shape)\n",
    "\n",
    "        # Calculate the batch loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Accumulate the total loss for this epoch\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Print training progress every 2000 batches\n",
    "        if batch_idx % 2000 == 0:\n",
    "            print(f'Iteration: {batch_idx+1}, Epoch: {epoch+1}, Loss: {total_loss/(batch_idx+1)}')\n",
    "\n",
    "        # Zero the gradients, backward pass, and update model parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ba9856152d44a398613d78ed435bf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Epoch: 1, Loss: 0.7347862124443054\n",
      "Iteration: 2001, Epoch: 1, Loss: 0.4168941239709559\n",
      "Iteration: 4001, Epoch: 1, Loss: 0.39521023140039124\n",
      "Iteration: 6001, Epoch: 1, Loss: 0.37919516868763936\n",
      "Iteration: 8001, Epoch: 1, Loss: 0.36901192133344124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2844816c1994176a2fc1195026e3726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Epoch: 2, Loss: 0.06954051554203033\n",
      "Iteration: 2001, Epoch: 2, Loss: 0.2920502882091508\n",
      "Iteration: 4001, Epoch: 2, Loss: 0.28847278226832695\n",
      "Iteration: 6001, Epoch: 2, Loss: 0.2878993847558715\n",
      "Iteration: 8001, Epoch: 2, Loss: 0.28795734991574046\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e27833b7371426dba30e445dd235be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Epoch: 3, Loss: 0.6256917119026184\n",
      "Iteration: 2001, Epoch: 3, Loss: 0.2293240738355893\n",
      "Iteration: 4001, Epoch: 3, Loss: 0.2287079924789705\n",
      "Iteration: 6001, Epoch: 3, Loss: 0.2282660641130001\n",
      "Iteration: 8001, Epoch: 3, Loss: 0.22693074686964212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4cba8a4bef4e28b68c3700d094feea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Epoch: 4, Loss: 0.18093843758106232\n",
      "Iteration: 2001, Epoch: 4, Loss: 0.16886994974880382\n",
      "Iteration: 4001, Epoch: 4, Loss: 0.16798559383514097\n",
      "Iteration: 6001, Epoch: 4, Loss: 0.16851474634214797\n",
      "Iteration: 8001, Epoch: 4, Loss: 0.169117067494951\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 4\n",
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid():\n",
    "    model.eval()\n",
    "    req_targets = []\n",
    "    req_outputs = []\n",
    "    valid_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for unw, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            req_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            req_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    valid_loss /= len(testing_loader)\n",
    "    return req_outputs, req_targets, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "outputs, targets, valid_loss = valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.array(outputs)\n",
    "targets = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_outputs = np.zeros((outputs.shape[0], outputs.shape[1]))\n",
    "\n",
    "for row in range(outputs.shape[0]):\n",
    "    for col in range(outputs.shape[1]):\n",
    "        if outputs[row][col] >= 0.5: int_outputs[row][col] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.38124305, 0.96169668, 0.013903  , 0.01263638, 0.02965521,\n",
       "        0.04526918, 0.06507529, 0.00927497, 0.02327961, 0.04501526]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0], int_outputs[0], outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_ham_loss = hamming_loss(targets, int_outputs)\n",
    "bert_jacc_score = jaccard_score(targets, int_outputs, average = 'samples')\n",
    "bert_lrap = label_ranking_average_precision_score(targets, outputs)\n",
    "bert_f1_macro = f1_score(targets, int_outputs, average = 'macro')\n",
    "bert_f1_micro = f1_score(targets, int_outputs, average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.39641014736425134\n",
      "Hamming Loss: 0.147\n",
      "Jaccard Score: 0.49601666666666666\n",
      "Label Ranking Average Precision Score: 0.7532342063492077\n",
      "F1 Macro Score: 0.5362393727899673\n",
      "F1 Micro Score: 0.5758799769186381\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Loss:\", valid_loss)\n",
    "print(\"Hamming Loss:\", bert_ham_loss)\n",
    "print(\"Jaccard Score:\", bert_jacc_score)\n",
    "print(\"Label Ranking Average Precision Score:\", bert_lrap)\n",
    "print(\"F1 Macro Score:\", bert_f1_macro)\n",
    "print(\"F1 Micro Score:\", bert_f1_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f = '/Users/neo/Documents/bert/bertmodel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT(\n",
       "  (layer1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Dropout(p=0.3, inplace=False)\n",
       "  (layer3): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = torch.load(\"/Users/neo/Documents/bert/bertmodel.pth\")\n",
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38133 entries, 0 to 38132\n",
      "Data columns (total 3 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   sectionName         38133 non-null  object\n",
      " 1   bodyContent         37938 non-null  object\n",
      " 2   webPublicationDate  38133 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 893.9+ KB\n"
     ]
    }
   ],
   "source": [
    "news = pd.read_csv('/Users/neo/Documents/bert/bert_filtered_final.csv')\n",
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df = pd.DataFrame()\n",
    "bert_df['Tweet']=news['bodyContent']\n",
    "values = [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] * 38133\n",
    "bert_df['list'] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(bert_df, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test_params = {'batch_size': 1,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': 0\n",
    "                    }   \n",
    "\n",
    "test_loader = DataLoader(test_dataset, **bert_test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    bert.eval()\n",
    "    bert_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for unw, data in enumerate(test_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = bert(ids, mask, token_type_ids)\n",
    "\n",
    "            bert_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "    return bert_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_outputs = np.array(test_outputs)\n",
    "\n",
    "for i in range(test_outputs.shape[0]):\n",
    "    for j in range(test_outputs.shape[1]):\n",
    "        if test_outputs[i][j] >= 0.5: test_outputs[i][j] = 1\n",
    "        else: test_outputs[i][j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df['Optimistic'] = \"None\"\n",
    "bert_df['Thankful'] = \"None\"\n",
    "bert_df['Empathetic'] = \"None\"\n",
    "bert_df['Pessimistic'] = \"None\"\n",
    "bert_df['Anxious'] = \"None\"\n",
    "bert_df['Sad'] = \"None\"\n",
    "bert_df['Annoyed'] = \"None\"\n",
    "bert_df['Denial'] = \"None\"\n",
    "bert_df['Official report'] = \"None\"\n",
    "bert_df['Joking'] = \"None\"\n",
    "bert_df = bert_df.drop(['list'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_outputs)):\n",
    "    bert_df['Optimistic'].iloc[i] = test_outputs[i][0]\n",
    "    bert_df['Thankful'].iloc[i] = test_outputs[i][1]\n",
    "    bert_df['Empathetic'].iloc[i] = test_outputs[i][2]\n",
    "    bert_df['Pessimistic'].iloc[i] = test_outputs[i][3]\n",
    "    bert_df['Anxious'].iloc[i] = test_outputs[i][4]\n",
    "    bert_df['Sad'].iloc[i] = test_outputs[i][5]\n",
    "    bert_df['Annoyed'].iloc[i] = test_outputs[i][6]\n",
    "    bert_df['Denial'].iloc[i] = test_outputs[i][7]\n",
    "    bert_df['Official report'].iloc[i] = test_outputs[i][8]\n",
    "    bert_df['Joking'].iloc[i] = test_outputs[i][9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_df['webPublicationDate'] = news['webPublicationDate']\n",
    "bert_df['sectionName'] = news['sectionName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Optimistic</th>\n",
       "      <th>Thankful</th>\n",
       "      <th>Empathetic</th>\n",
       "      <th>Pessimistic</th>\n",
       "      <th>Anxious</th>\n",
       "      <th>Sad</th>\n",
       "      <th>Annoyed</th>\n",
       "      <th>Denial</th>\n",
       "      <th>Official report</th>\n",
       "      <th>Joking</th>\n",
       "      <th>webPublicationDate</th>\n",
       "      <th>sectionName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38033</th>\n",
       "      <td>Rishi Sunak’s attempt to cast himself as a tax...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>UK news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38034</th>\n",
       "      <td>The Victorian Liberal MP Wendy Lovell has come...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>Australia news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38035</th>\n",
       "      <td>The prime minister, Scott Morrison, has distan...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>Australia news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38036</th>\n",
       "      <td>Next week’s budget is guaranteed to be all thi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38037</th>\n",
       "      <td>A group of 54 Ukrainian orphans fleeing the ho...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-24</td>\n",
       "      <td>UK news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38128</th>\n",
       "      <td>Top story: ‘They want to raze it to the ground...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>World news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38129</th>\n",
       "      <td>A Russian couple who staged a solitary protest...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>UK news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38130</th>\n",
       "      <td>All big experiences in our lives have two real...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>Opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38131</th>\n",
       "      <td>Until the Russian invasion of Ukraine, Hungary...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>World news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38132</th>\n",
       "      <td>The lines for sugar in Saratov were hard not t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-03-23</td>\n",
       "      <td>World news</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Tweet Optimistic Thankful  \\\n",
       "38033  Rishi Sunak’s attempt to cast himself as a tax...        0.0      0.0   \n",
       "38034  The Victorian Liberal MP Wendy Lovell has come...        0.0      0.0   \n",
       "38035  The prime minister, Scott Morrison, has distan...        0.0      0.0   \n",
       "38036  Next week’s budget is guaranteed to be all thi...        0.0      0.0   \n",
       "38037  A group of 54 Ukrainian orphans fleeing the ho...        0.0      1.0   \n",
       "...                                                  ...        ...      ...   \n",
       "38128  Top story: ‘They want to raze it to the ground...        0.0      0.0   \n",
       "38129  A Russian couple who staged a solitary protest...        0.0      0.0   \n",
       "38130  All big experiences in our lives have two real...        0.0      0.0   \n",
       "38131  Until the Russian invasion of Ukraine, Hungary...        0.0      0.0   \n",
       "38132  The lines for sugar in Saratov were hard not t...        0.0      0.0   \n",
       "\n",
       "      Empathetic Pessimistic Anxious  Sad Annoyed Denial Official report  \\\n",
       "38033        0.0         0.0     0.0  0.0     0.0    0.0             1.0   \n",
       "38034        0.0         0.0     0.0  0.0     1.0    0.0             1.0   \n",
       "38035        0.0         0.0     0.0  0.0     0.0    0.0             1.0   \n",
       "38036        0.0         1.0     0.0  0.0     0.0    1.0             0.0   \n",
       "38037        0.0         0.0     0.0  0.0     0.0    0.0             1.0   \n",
       "...          ...         ...     ...  ...     ...    ...             ...   \n",
       "38128        0.0         0.0     1.0  0.0     1.0    1.0             1.0   \n",
       "38129        0.0         0.0     0.0  0.0     0.0    0.0             1.0   \n",
       "38130        0.0         1.0     0.0  0.0     0.0    0.0             0.0   \n",
       "38131        0.0         0.0     0.0  0.0     0.0    0.0             1.0   \n",
       "38132        0.0         0.0     1.0  0.0     0.0    0.0             0.0   \n",
       "\n",
       "      Joking webPublicationDate     sectionName  \n",
       "38033    0.0         2022-03-24         UK news  \n",
       "38034    0.0         2022-03-24  Australia news  \n",
       "38035    0.0         2022-03-24  Australia news  \n",
       "38036    0.0         2022-03-24        Business  \n",
       "38037    0.0         2022-03-24         UK news  \n",
       "...      ...                ...             ...  \n",
       "38128    0.0         2022-03-23      World news  \n",
       "38129    0.0         2022-03-23         UK news  \n",
       "38130    0.0         2022-03-23         Opinion  \n",
       "38131    0.0         2022-03-23      World news  \n",
       "38132    0.0         2022-03-23      World news  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_df.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_df.to_csv(\"/Users/neo/Documents/bert/bert_final2.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
