{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.util import bigrams, trigrams\n",
    "from nltk.corpus import stopwords\n",
    "import unicodedata\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    # 加载英文停用词集合\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # 要添加的额外词语列表\n",
    "    # extra_stop_words = ['guardian', 'news', 'guardian.com', 'say', 'said', 'article']\n",
    "    extra_stop_words = [\n",
    "        'guardian', 'article', 'theguardian', 'com', 'says', 'said', 'just', 'like', 'can', 'one', 'also',\n",
    "        'year', 'years', 'time', 'times', 'world', 'make', 'makes', 'made', 'know', 'known', 'go','south',\n",
    "        'going', 'get', 'getting', 'got', 'see', 'seeing', 'seen', 'may', 'might', 'week', 'weeks', 'month', 'months',\n",
    "        'use', 'used', 'using', 'think', 'thinks', 'thought', 'take', 'takes', 'took', 'come', 'comes', 'came',\n",
    "        'way', 'ways', 'many', 'much', 'news', 'report', 'including', 'use', 'good', 'bad', 'look', 'looks',\n",
    "        'looking', 'help', 'want', 'wants', 'wanted', 'need', 'needs', 'needed', 'important', 'lot', 'lots', 'tell',\n",
    "        'tells', 'told', 'work', 'works', 'worked', 'place', 'places', 'point', 'points', 'number', 'numbers',\n",
    "        'group', 'groups', 'man', 'men', 'woman', 'women', 'child', 'children', 'company', 'companies', 'zealand',\n",
    "        'york'\n",
    "    ]\n",
    "\n",
    "    stop_words.update(extra_stop_words)\n",
    "\n",
    "    # 将额外词语添加到停用词集合中\n",
    "    stop_words.update(extra_stop_words)\n",
    "    # stop_words = set(stopwords.words('english')) #+ ['guardian', 'news', 'theguardian.com', 'say', 'said', 'article', '·', '-']\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "            .encode('ascii', 'ignore')\n",
    "            .decode('utf-8', 'ignore')\n",
    "            .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改filter_and_save_data函数，返回过滤后的DataFrame\n",
    "def filter_data(df, start_date, end_date, section_names):\n",
    "    df['webPublicationDate'] = pd.to_datetime(df['webPublicationDate'])\n",
    "    return df[(df['webPublicationDate'] >= start_date) & (df['webPublicationDate'] <= end_date) & (df['sectionName'].isin(section_names))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_names = ['Australia news']\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2022-03-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/neo/Documents/bert/bert_final2.csv')\n",
    "filtered_data = filter_data(data, start_date, end_date, section_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams: [('nope', 'banking'), ('banking', 'royal'), ('royal', 'commissioner'), ('commissioner', 'kenneth'), ('kenneth', 'haynes'), ('haynes', 'response'), ('response', 'asked'), ('asked', 'shake'), ('shake', 'hand'), ('hand', 'treasurer')]\n",
      "Trigrams: [('nope', 'banking', 'royal'), ('banking', 'royal', 'commissioner'), ('royal', 'commissioner', 'kenneth'), ('commissioner', 'kenneth', 'haynes'), ('kenneth', 'haynes', 'response'), ('haynes', 'response', 'asked'), ('response', 'asked', 'shake'), ('asked', 'shake', 'hand'), ('shake', 'hand', 'treasurer'), ('hand', 'treasurer', 'josh')]\n"
     ]
    }
   ],
   "source": [
    "all_bigrams = []\n",
    "all_trigrams = []\n",
    "for text in filtered_data['Tweet']:\n",
    "    # 确保text是字符串类型\n",
    "    cleaned_text = clean(str(text))\n",
    "    all_bigrams.extend(list(bigrams(cleaned_text)))\n",
    "    all_trigrams.extend(list(trigrams(cleaned_text)))\n",
    "# 分析bigrams和trigrams\n",
    "print(\"Bigrams:\", all_bigrams[:10]) # 打印前10个bigrams作为示例\n",
    "print(\"Trigrams:\", all_trigrams[:10]) # 打印前10个trigrams作为示例"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
